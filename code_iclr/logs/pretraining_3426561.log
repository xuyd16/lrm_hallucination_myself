GpuFreq=control_disabled
GpuFreq=control_disabled
GpuFreq=control_disabled
GpuFreq=control_disabled
DDP初始化成功: rank=0, local_rank=0, world_size=16
使用设备: cuda:0
可用GPU数量: 4
  GPU 0: NVIDIA H100
    显存: 93.1 GB
  GPU 1: NVIDIA H100
    显存: 93.1 GB
  GPU 2: NVIDIA H100
    显存: 93.1 GB
  GPU 3: NVIDIA H100
    显存: 93.1 GB

=== 模型配置信息 ===
num_regions: 5
max_electrodes_per_region: 24
sequence_length: 1600
embed_dim: 768
num_heads: 24
depth: 24
mlp_ratio: 4.0
drop_rate: 0.2
attn_drop_rate: 0.2
drop_path_rate: 0.2
use_abs_pos: False
use_rel_pos: True
use_time_embed: True
mask_ratio: 0.15
freq_mask_ratio: 0.3
mask_strategy: random
mask_noise_ratio: 0.005
use_moe: True
num_experts: 16
top_k_experts: 2
moe_aux_loss_coeff: 0.01
n_freq_bands: 5
freq_loss_weight: 0.1
time_loss_weight: 0.9
lr: 3e-05
weight_decay: 0.0001
warmup_epochs: 10
use_amp: True
clip_grad: 1.0
batch_size: 4
use_layer_norm: True
use_batch_norm: True
eps: 1e-08
init_std: 0.02
electrode_names: ['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'Fz', 'C3', 'C4', 'Cz', 'P3', 'P4', 'P7', 'P8', 'Pz', 'O1', 'O2', 'Oz', 'T3', 'T4', 'T5', 'T6']
region_names: ['frontal', 'central', 'parietal', 'temporal', 'occipital']
debug: False
freq_eval: True
use_channel_embed: True
channel_embed_dim: 32
num_brain_regions: 5
channel_gat_initial_dim: 32
intra_region_gat_heads: 4
intra_region_gat_dim_per_head: 32
region_agg_attention_dim: 64
inter_region_gat_heads: 4
inter_region_gat_dim_per_head: 64
模型已创建并移动到设备: cuda:0
模型已使用DDP封装在设备 0
优化器已创建，学习率: 3e-05
DDP训练，世界大小: 16, 有效批次大小: 64
动态梯度累积设置:
  目标全局批次大小: 128
  当前全局批次大小: 64
  梯度累积步数: 2
  实际有效全局批次大小: 128
训练器已创建

============================================================
                           模型参数统计                           
============================================================
总参数量:                1,892,973,456
可训练参数:               1,892,973,456
冻结参数:                0
参数大小:                7221.12 MB (FP32)
参数大小:                3610.56 MB (FP16)

------------------------------------------------------------
                          主要模块参数分布                          
------------------------------------------------------------
region_projection.raw_projection.0        2,459,136 (  0.1%)
fusion_module.mlp_t.0                     2,362,368 (  0.1%)
fusion_module.mlp_f.0                     2,362,368 (  0.1%)
layers.0.mlp.experts.0.0                  2,362,368 (  0.1%)
layers.0.mlp.experts.1.0                  2,362,368 (  0.1%)
layers.0.mlp.experts.2.0                  2,362,368 (  0.1%)
layers.0.mlp.experts.3.0                  2,362,368 (  0.1%)
layers.0.mlp.experts.4.0                  2,362,368 (  0.1%)
layers.0.mlp.experts.5.0                  2,362,368 (  0.1%)
layers.0.mlp.experts.6.0                  2,362,368 (  0.1%)
其他模块                                     1,865,708,682 ( 98.6%)
============================================================

正在加载数据: /hkfs/work/workspace/scratch/tum_fmp0582-ictspace/processed_data2
警告：在文件 BCI_Competition_IV-1_data.h5 中未找到标准数据集，已跳过。
警告：在文件 Emobrain_data_data.h5 中未找到标准数据集，已跳过。
警告：在文件 REEG-BACA_data.h5 中未找到标准数据集，已跳过。
警告：在文件 Raw_EEG_Data_data.h5 中未找到标准数据集，已跳过。
警告：在文件 Resting_State_EEG_Data_data.h5 中未找到标准数据集，已跳过。
警告：在文件 SEED-V_data.h5 中未找到标准数据集，已跳过。
警告：在文件 SEED_FRA_data.h5 中未找到标准数据集，已跳过。
警告：在文件 Siena_Scalp_EEG_Database_data.h5 中未找到标准数据集，已跳过。
警告：在文件 TDBrain_data.h5 中未找到标准数据集，已跳过。
警告：在文件 TUAR_data.h5 中未找到标准数据集，已跳过。
警告：在文件 TUEP_data.h5 中未找到标准数据集，已跳过。
警告：在文件 TUH1.h5 中未找到标准数据集，已跳过。
警告：在文件 TUH10.h5 中未找到标准数据集，已跳过。
警告：在文件 TUH2.h5 中未找到标准数据集，已跳过。
警告：在文件 TUH3.h5 中未找到标准数据集，已跳过。
警告：在文件 TUH4.h5 中未找到标准数据集，已跳过。
警告：在文件 TUH5.h5 中未找到标准数据集，已跳过。
警告：在文件 TUH6.h5 中未找到标准数据集，已跳过。
警告：在文件 TUH7.h5 中未找到标准数据集，已跳过。
警告：在文件 TUH8.h5 中未找到标准数据集，已跳过。
警告：在文件 TUH9.h5 中未找到标准数据集，已跳过。
警告：在文件 TUSL_data.h5 中未找到标准数据集，已跳过。
警告：在文件 TUSZ_data.h5 中未找到标准数据集，已跳过。
警告：在文件 Workload_data.h5 中未找到标准数据集，已跳过。
全局扫描完成: 共找到 25 个文件, 4558 个总样本。
DDP已启用，对 4558 个样本进行分片。Rank 0 分配到 285 个样本。
数据集初始化完成: 当前进程样本数: 285
警告: num_workers=24 可能过高，建议设置为16以下
数据加载配置: num_workers=24, 服务器CPU核心充足
数据加载器已创建: 样本数=285, 批次大小=4

==================================================
开始训练 DualDomain Neural Transformer MEM 模型
共100个周期，批次大小:4
==================================================

学习率调度器已初始化，总训练epochs: 100, 每个epoch的步数: 72
总训练步数: 7200
学习率调度器设置成功
警告: 未找到TensorBoard，将不会记录训练日志

==================================================
     DualDomain Neural Transformer MEM 模型训练开始     
==================================================
                     本次运行文件夹                      
--------------------------------------------------
运行文件夹:          checkpoint/20250816_235653
最佳模型:           checkpoint/20250816_235653/best_model.pt
最后模型:           checkpoint/20250816_235653/last_model.pt
--------------------------------------------------
                      模型参数统计                      
--------------------------------------------------
总参数量:           1,892,973,456
可训练参数:          1,892,973,456
冻结参数:           0
内存占用(FP32):     7221.1 MB
内存占用(FP16):     3610.6 MB

                     主要组件参数分布                     
--------------------------------------------------
layers               1,870,424,064 ( 98.8%)
fusion_module          15,357,696 (  0.8%)
region_projection       4,606,470 (  0.2%)
time_reconstruction_head1    1,230,400 (  0.1%)
time_reconstruction_head2    1,230,400 (  0.1%)
positional_embedding       92,160 (  0.0%)
intra_region_pos_embedding       18,432 (  0.0%)
freq_reconstruction_head1        3,845 (  0.0%)
freq_reconstruction_head2        3,845 (  0.0%)
region_embedding            3,840 (  0.0%)
norm                        1,536 (  0.0%)

                      训练配置信息                      
--------------------------------------------------
训练周期:           100
批次大小:           4
有效批次大小:         64
设备:             cuda:0
DDP世界大小:        16
DDP模式:          启用
学习率:            3e-05
时域损失权重:         0.9
频域损失权重:         0.1
混合精度:           启用
梯度累积步数:         2
每个周期的步数:        72
==================================================


Epoch 1/100
--------------------------------------------------
序列长度 (R*E): 120
时域预测形状: torch.Size([4, 120, 1600]), 目标形状: torch.Size([4, 120, 1600])
频域预测形状: torch.Size([4, 120, 5]), 目标形状: torch.Size([4, 120, 5])
训练进度: [=                   ] 1/72 | 总损失: 0.66932440 | 时域: 1.07152247 | 频域: 17.21595764 | LR: 0.00000060 | 用时: 3.6秒训练进度: [=                   ] 4/72 | 总损失: 0.65167665 | 时域: 1.06883216 | 频域: 16.62270927 | LR: 0.00000060 | 用时: 6.5秒训练进度: [==                  ] 7/72 | 总损失: 0.62634057 | 时域: 1.07002413 | 频域: 15.55492783 | LR: 0.00000060 | 用时: 8.8秒训练进度: [===                 ] 10/72 | 总损失: 0.62502325 | 时域: 1.07460451 | 频域: 15.42165375 | LR: 0.00000060 | 用时: 11.1秒训练进度: [====                ] 13/72 | 总损失: 0.60389853 | 时域: 1.06403089 | 频域: 14.58824348 | LR: 0.00000060 | 用时: 13.4秒训练进度: [=====               ] 16/72 | 总损失: 0.60260338 | 时域: 1.07952833 | 频域: 14.50930405 | LR: 0.00000060 | 用时: 15.8秒训练进度: [======              ] 19/72 | 总损失: 0.59605873 | 时域: 1.07704997 | 频域: 14.19055462 | LR: 0.00000060 | 用时: 18.1秒训练进度: [======              ] 22/72 | 总损失: 0.58724642 | 时域: 1.06865382 | 频域: 13.95632648 | LR: 0.00000060 | 用时: 20.4秒训练进度: [=======             ] 25/72 | 总损失: 0.58737719 | 时域: 1.08740735 | 频域: 13.73164272 | LR: 0.00000060 | 用时: 22.7秒训练进度: [========            ] 28/72 | 总损失: 0.56665289 | 时域: 1.04672599 | 频域: 13.10658169 | LR: 0.00000060 | 用时: 25.1秒训练进度: [=========           ] 31/72 | 总损失: 0.57722777 | 时域: 1.09000719 | 频域: 13.40768433 | LR: 0.00000060 | 用时: 27.4秒训练进度: [==========          ] 34/72 | 总损失: 0.56566823 | 时域: 1.05490661 | 频域: 13.18337059 | LR: 0.00000060 | 用时: 29.7秒训练进度: [===========         ] 37/72 | 总损失: 0.56857467 | 时域: 1.08567977 | 频域: 13.13770771 | LR: 0.00000060 | 用时: 32.0秒训练进度: [===========         ] 40/72 | 总损失: 0.56199813 | 时域: 1.06911862 | 频域: 12.99313068 | LR: 0.00000060 | 用时: 34.5秒训练进度: [============        ] 43/72 | 总损失: 0.56231534 | 时域: 1.07065320 | 频域: 13.14426231 | LR: 0.00000060 | 用时: 36.8秒训练进度: [=============       ] 46/72 | 总损失: 0.55606210 | 时域: 1.06492257 | 频域: 12.80222321 | LR: 0.00000060 | 用时: 39.2秒训练进度: [==============      ] 49/72 | 总损失: 0.54293406 | 时域: 1.04631829 | 频域: 12.46550941 | LR: 0.00000060 | 用时: 41.5秒训练进度: [===============     ] 52/72 | 总损失: 0.55785638 | 时域: 1.05639541 | 频域: 13.16666794 | LR: 0.00000060 | 用时: 44.1秒训练进度: [================    ] 55/72 | 总损失: 0.54685569 | 时域: 1.05248356 | 频域: 12.74370480 | LR: 0.00000060 | 用时: 46.5秒训练进度: [================    ] 58/72 | 总损失: 0.55098861 | 时域: 1.05461860 | 频域: 12.90493202 | LR: 0.00000060 | 用时: 48.9秒训练进度: [=================   ] 61/72 | 总损失: 0.55596256 | 时域: 1.10174072 | 频域: 12.90982914 | LR: 0.00000061 | 用时: 51.2秒训练进度: [=================[rank14]:[E ProcessGroupNCCL.cpp:563] [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=20412, OpType=ALLREDUCE, NumelIn=5, NumelOut=5, Timeout(ms)=600000) ran for 600060 milliseconds before timing out.
[rank15]:[E ProcessGroupNCCL.cpp:563] [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=20412, OpType=ALLREDUCE, NumelIn=5, NumelOut=5, Timeout(ms)=600000) ran for 600062 milliseconds before timing out.
[rank0]:[E ProcessGroupNCCL.cpp:563] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=20414, OpType=ALLREDUCE, NumelIn=9290, NumelOut=9290, Timeout(ms)=600000) ran for 600029 milliseconds before timing out.
[rank2]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 2] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=15
[rank2]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 2] ProcessGroupNCCL preparing to dump debug info.
[rank2]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 2] [PG 0 Rank 2] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 15
[rank1]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 1] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=8
[rank1]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 1] ProcessGroupNCCL preparing to dump debug info.
[rank1]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 1] [PG 0 Rank 1] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 8
[rank3]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 3] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=43
[rank3]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 3] ProcessGroupNCCL preparing to dump debug info.
[rank3]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 3] [PG 0 Rank 3] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 43
[rank0]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 0] Timeout at NCCL work: 20414, last enqueued NCCL work: 20414, last completed NCCL work: 20413.
[rank0]:[E ProcessGroupNCCL.cpp:577] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E ProcessGroupNCCL.cpp:583] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=20414, OpType=ALLREDUCE, NumelIn=9290, NumelOut=9290, Timeout(ms)=600000) ran for 600029 milliseconds before timing out.
Exception raised from checkTimeout at /opt/conda/conda-bld/pytorch_1716905971093/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x152bd7fbb897 in /home/hk-project-p0022560/tum_fmp0582/anaconda3/envs/myenv1/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x152bd9286d12 in /home/hk-project-p0022560/tum_fmp0582/anaconda3/envs/myenv1/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1a0 (0x152bd928bb30 in /home/hk-project-p0022560/tum_fmp0582/anaconda3/envs/myenv1/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x152bd928ce7c in /home/hk-project-p0022560/tum_fmp0582/anaconda3/envs/myenv1/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbbf4 (0x152c39821bf4 in /home/hk-project-p0022560/tum_fmp0582/anaconda3/envs/myenv1/lib/python3.9/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #5: <unknown function> + 0x89c0a (0x152c4ec89c0a in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10ec60 (0x152c4ed0ec60 in /lib64/libc.so.6)

[rank6]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 6] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=9
[rank6]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 6] ProcessGroupNCCL preparing to dump debug info.
[rank6]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 6] [PG 0 Rank 6] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 9
[rank12]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 12] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=16
[rank12]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 12] ProcessGroupNCCL preparing to dump debug info.
[rank12]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 12] [PG 0 Rank 12] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 16
[rank13]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 13] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=16
[rank13]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 13] ProcessGroupNCCL preparing to dump debug info.
[rank13]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 13] [PG 0 Rank 13] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 16
[rank14]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 14] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=1
[rank14]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 14] ProcessGroupNCCL preparing to dump debug info.
[rank14]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 14] [PG 0 Rank 14] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 1
[rank7]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 7] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=16
[rank7]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 7] ProcessGroupNCCL preparing to dump debug info.
[rank7]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 7] [PG 0 Rank 7] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 16
[rank4]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 4] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=16
[rank4]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 4] ProcessGroupNCCL preparing to dump debug info.
[rank4]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 4] [PG 0 Rank 4] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 16
[rank5]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 5] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=16
[rank5]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 5] ProcessGroupNCCL preparing to dump debug info.
[rank5]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 5] [PG 0 Rank 5] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 16
[rank15]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 15] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=1
[rank15]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 15] ProcessGroupNCCL preparing to dump debug info.
[rank15]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 15] [PG 0 Rank 15] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 1
[rank10]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 10] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=1
[rank10]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 10] ProcessGroupNCCL preparing to dump debug info.
[rank10]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 10] [PG 0 Rank 10] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 1
[rank11]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 11] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=16
[rank11]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 11] ProcessGroupNCCL preparing to dump debug info.
[rank11]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 11] [PG 0 Rank 11] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 16
[rank8]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 8] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=16
[rank8]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 8] ProcessGroupNCCL preparing to dump debug info.
[rank8]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 8] [PG 0 Rank 8] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 16
[rank9]:[E ProcessGroupNCCL.cpp:1316] [PG 0 Rank 9] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=1
[rank9]:[E ProcessGroupNCCL.cpp:1153] [PG 0 Rank 9] ProcessGroupNCCL preparing to dump debug info.
[rank9]:[F ProcessGroupNCCL.cpp:1169] [PG 0 Rank 9] [PG 0 Rank 9] ProcessGroupNCCL's watchdog got stuck for 1200 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 1
=  ] 64/72 | 总损失: 0.54231656 | 时域: 1.04277194 | 频域: 12.69901371 | LR: 0.00000061 | 用时: 53.7秒训练进度: [=================== ] 67/72 | 总损失: 0.53214747 | 时域: 1.02591002 | 频域: 12.44331169 | LR: 0.00000061 | 用时: 56.0秒训练进度: [====================] 70/72 | 总损失: 0.54047763 | 时域: 1.05120921 | 频域: 12.60814571 | LR: 0.00000061 | 用时: 58.5秒srun: error: hkn0912: tasks 0-3: Aborted (core dumped)
srun: error: hkn0918: tasks 4-7: Aborted (core dumped)
srun: error: hkn0919: tasks 8-11: Aborted (core dumped)
srun: error: hkn0921: tasks 12-15: Aborted (core dumped)
